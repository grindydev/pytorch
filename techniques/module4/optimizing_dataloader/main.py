import gc
import os
import json

import torch
from torch.utils.data import DataLoader

import helper_utils
from pathlib import Path

if torch.cuda.is_available():
    device = torch.device('cuda')
elif torch.backends.mps.is_available():
    device = torch.device('mps')
else:
    device = torch.device('cpu')

print(f"Using device: {device}")

data_path = Path.cwd() / "data/cifar10"

trainset = helper_utils.download_and_load_cifar10(data_path)

cpu_cores = os.cpu_count()
print(f"Number of available CPU cores: {cpu_cores}")

# Define the list of num_workers values to test
workers_to_test = [0, 2, 4, 6, 8, 10]

def experiment_workers(workers_to_test, trainset, device):
    """
    Measures the data loading time for different numbers of workers.

    Args:
        workers_to_test: A list of integers representing the number of workers to test.
        trainset: The dataset to be loaded.
        device: The device to which the data will be moved (e.g., 'cpu' or 'cuda').
    """
    # Initialize a dictionary to store the results
    worker_times = {}

    # Loop through each worker number you want to test
    for nw in workers_to_test:
        print(f"--- Testing Number of Workers = {nw} ---")
        
        # Create a new DataLoader instance for each specific test.
        loader = DataLoader(trainset, 
                            batch_size=32, 
                            shuffle=True,
                            # The 'num_workers' is set to the current value in the loop.
                            num_workers=nw
                        )
        
        # Handle potential runtime errors
        try:
            # Time the data loading for one epoch and save it to the dictionary
            worker_times[nw] = helper_utils.measure_average_epoch_time(loader, device)
        except RuntimeError as e:
            # If an error occurs (often from running out of shared memory)
            print(f"\n❌ ERROR with {nw} workers. Likely a shared memory issue.")
            worker_times[nw] = float('inf')
            
        # Clean up the loader and call the garbage collector to free up memory
        del loader
        gc.collect()

        # Clear the PyTorch CUDA cache to free up GPU memory
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

    return worker_times

# Run the experiment to measure the data loading time for different numbers of workers.
worker_times = helper_utils.run_experiment(
    # A unique name for this experiment, used as the filename for the cached results.
    experiment_name='worker_times', 
    # The actual function that contains the experiment's logic.
    experiment_fcn=experiment_workers, 
    # The parameters to iterate over; in this case, a list of worker counts.
    cases=workers_to_test, 
    # The dataset required by the experiment function.
    trainset=trainset, 
    # The computation device (e.g., 'cpu' or 'cuda') to be used.
    device=device,
    # If False, the function will load results from the cache if they exist.
    # If True, it will force the experiment to run again and overwrite any old results.
    rerun=False
)


helper_utils.plot_performance_summary(
    worker_times, 
    title="DataLoader Performance vs. num_workers", 
    xlabel="Number of Workers", 
    ylabel="Average Time per Epoch (milliseconds)"
)

### This cell will take a few seconds to run

# Create the dictionary of loaders iteratively using a dictionary comprehension
# for each number in the 'workers_to_test' list.
loaders_to_compare = {
    f"{nw} Workers": DataLoader(trainset, batch_size=32, num_workers=nw) 
    for nw in workers_to_test
}

# Pass the generated dictionary to the plotting function.
helper_utils.visualize_dataloader_efficiency(loaders_to_compare, device)

# Clean up and release memory.
del loaders_to_compare
gc.collect()

# Clear the PyTorch CUDA cache to free up GPU memory.
if torch.cuda.is_available():
    torch.cuda.empty_cache()

# Define the list of batch_size values to test
batch_sizes_to_test = [16, 32, 64, 128, 256, 512]
num_workers = 0 # testing different workers on your device

def experiment_batch_sizes(batch_sizes_to_test, trainset, device):
    """
    Measures the data loading time for different batch sizes.

    Args:
        batch_sizes_to_test: A list of integers representing the batch sizes to test.
        trainset: The dataset to be loaded.
        device: The device to which the data will be moved (e.g., 'cpu' or 'cuda').
    """
    # Initialize a dictionary to store the results
    batch_size_times = {}

    # Loop through each batch size you want to test
    for bs in batch_sizes_to_test:
        print(f"--- Testing Batch Size = {bs} ---")
        
        # Create a new DataLoader instance for each specific test.
        loader = DataLoader(trainset, 
                            # The 'batch_size' is set to the current value in the loop.
                            batch_size=bs, 
                            shuffle=True,
                            num_workers=num_workers
                        )
        
        # Handle potential runtime errors, especially out-of-memory
        try:
            # Time the data loading for one epoch and save it to the dictionary
            batch_size_times[bs] = helper_utils.measure_average_epoch_time(loader, device)
        except RuntimeError as e:
            # If an error occurs (often from running out of GPU memory),
            print(f"\n❌ ERROR with batch size {bs}. Likely a GPU memory issue.")
            batch_size_times[bs] = float('inf')
            
        # Clean up the loader and call the garbage collector to free up memory
        # ensuring each test runs in a clean environment.
        del loader
        gc.collect()

        # Clear the PyTorch CUDA cache to free up GPU memory.
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
    return batch_size_times


# Run the experiment to measure the data loading time for different batch sizes.
batch_size_times = helper_utils.run_experiment(
    # A unique name for this experiment, used as the filename for the cached results.
    experiment_name="batch_size_times", 
    # The actual function that contains the experiment's logic.
    experiment_fcn=experiment_batch_sizes,
    # The parameters to iterate over; in this case, a list of different batch sizes.
    cases=batch_sizes_to_test,
    # The dataset required by the experiment function.
    trainset=trainset,
    # The computation device (e.g., 'cpu' or 'cuda') to be used.
    device=device,
    # If False, the function will load results from the cache if they exist.
    # If True, it will force the experiment to run again and overwrite any old results.
    rerun=False
)

helper_utils.plot_performance_summary(
    batch_size_times, 
    title="DataLoader Performance vs. batch_size", 
    xlabel="Batch Sizes", 
    ylabel="Average Time per Epoch (milliseconds)"
)

pin_memory_settings = [False, True]

def experiment_pin_memory(pin_memory_settings, trainset, device):
    """
    Measures the data loading time with and without pinned memory.

    Args:
        pin_memory_settings: A list of boolean values to test for pin_memory.
        trainset: The dataset to be loaded.
        device: The device to which the data will be moved (e.g., 'cpu' or 'cuda').
    """
    # Initialize a dictionary to store the results
    pin_memory_times = {}

    # Loop through each pin_memory setting
    for setting in pin_memory_settings:
        print(f"--- Testing with pin_memory = {setting} ---")
        
        # Create a DataLoader with the current pin_memory setting
        loader = DataLoader(trainset,
                            batch_size=256,
                            num_workers=6,
                            shuffle=True,
                            # The 'pin_memory' is set to the current boolean value in the loop.
                            pin_memory=setting
                        )
        
        try:
            # Measure performance and store the result in the dictionary
            pin_memory_times[setting] = helper_utils.measure_average_epoch_time(loader, device)
        except RuntimeError as e:
            # Print an error message if an exception occurs
            print(f"\n❌ An error occurred with pin_memory = {setting}: {e}")
            pin_memory_times[setting] = float('inf')
            
        # --- Memory Cleanup for each iteration ---
        del loader
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

    return pin_memory_times


# Run the experiment to measure the data loading time for pin memory either set as False or True.
pin_memory_times = helper_utils.run_experiment(
    # A unique name for this experiment, used as the filename for the cached results.
    experiment_name="pin_memory_times",
    # The actual function that contains the experiment's logic.
    experiment_fcn=experiment_pin_memory,
    # The parameters to iterate over; in this case, a list of boolean values for pin memory.
    cases=pin_memory_settings,
    # The dataset required by the experiment function.
    trainset=trainset, 
    # The computation device (e.g., 'cpu' or 'cuda') to be used.
    device=device,
    # If False, the function will load results from the cache if they exist.
    # If True, it will force the experiment to run again and overwrite any old results.
    rerun=False
)

helper_utils.plot_performance_summary(
    pin_memory_times, 
    title="DataLoader Performance vs. pin_memory", 
    xlabel="Pin Memory", 
    ylabel="Average Time per Epoch (milliseconds)"
)

# Define the list of prefetch_factor values to test
prefetch_factors_to_test = [2, 4, 6, 8, 10, 12]

def experiment_prefetch_factor(prefetch_factors_to_test, trainset, device):
    """
    Measures the data loading time for different prefetch factor settings.

    Args:
        prefetch_factors_to_test: A list of integers representing the prefetch factors to test.
        trainset: The dataset to be loaded.
        device: The device to which the data will be moved (e.g., 'cpu' or 'cuda').
    """
    # Initialize a dictionary to store the results
    prefetch_factor_times = {}

    # Loop through each prefetch factor you want to test
    for pf in prefetch_factors_to_test:
        print(f"--- Testing prefetch_factor = {pf} ---")
        
        # Create a new DataLoader instance for each specific test, using the optimal settings
        loader = DataLoader(trainset, 
                            batch_size=256, 
                            shuffle=True,
                            num_workers=6,
                            pin_memory=False,
                            # The 'prefetch_factor' is set to the current value in the loop.
                            prefetch_factor=pf
                        )
        
        # Handle potential runtime errors
        try:
            # Time the data loading for one epoch and save it to the dictionary
            prefetch_factor_times[pf] = helper_utils.measure_average_epoch_time(loader, device)
        except RuntimeError as e:
            # If an error occurs, record it.
            print(f"\n❌ ERROR with prefetch_factor {pf}: {e}")
            prefetch_factor_times[pf] = float('inf')
            
        # Clean up the loader and call the garbage collector to free up memory
        # ensuring each test runs in a clean environment.
        del loader
        gc.collect()

        # Clear the PyTorch CUDA cache to free up GPU memory.
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

    return prefetch_factor_times

# Run the experiment to measure the data loading time for different prefetch factor.
prefetch_factor_times = helper_utils.run_experiment(
    # A unique name for this experiment, used as the filename for the cached results.
    experiment_name="prefetch_factor_times", 
    # The actual function that contains the experiment's logic.
    experiment_fcn=experiment_prefetch_factor,
    # The parameters to iterate over; in this case, a list of different prefetch factor.
    cases=prefetch_factors_to_test,
    # The dataset required by the experiment function.
    trainset=trainset, 
    # The computation device (e.g., 'cpu' or 'cuda') to be used.
    device=device,
    # If False, the function will load results from the cache if they exist.
    # If True, it will force the experiment to run again and overwrite any old results.
    rerun=False
)

helper_utils.plot_performance_summary(
    prefetch_factor_times, 
    title="DataLoader Performance vs. prefetch_factor", 
    xlabel="Prefetch Factor", 
    ylabel="Average Time per Epoch (milliseconds)"
)
